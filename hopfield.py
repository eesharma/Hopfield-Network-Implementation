# -*- coding: utf-8 -*-
"""hopfield.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17t86mWrodK_vkUUAMhEByjWQwIAwgrMH

# Hopfield Networks

In this project, we will explore Hopfield Networks, a learned method for encoding data. Given a dataset consisting of $n$ nodes, it learns a total of $n^2 - n$ connections, one between each pair. A given node's activation (pre-nonlinearity) is simply the inner product between its weight vector and the states of the other nodes. Because there are recurrent connections between nodes, the "forward pass" needs to be repeated multiple times until an attractor state is reached. Hopefield Networks are interesting in neuroscience as they provide a potentially biologically-feasible approach to memory. In this project, we will implement Hopfield Networks, explore their ability to store data and investigate their robustness to data corruptions. 

We'll start with a basic implementation below. See in-line comments for explanations of the functions. These allows us to run (synchronous) dynamics of a Hopfield Network.
"""

import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import moviepy.editor
from tqdm import tqdm
import os


def run(num_patterns, pattern_res, iters=500, K=0, out_path=None, return_weights=False):
    # Creates a batch of random patterns, (optionally) corrupts them and then
    # trains a Hopfield Network via Hebb's Rule to store the data.
    patterns = np.random.choice([-1.0, 1.0], size=(num_patterns, pattern_res, pattern_res))
    weights = learn_hopfield_net(patterns)
    if K > 0:  # Optionally corrupt the patterns:
        patterns_in = corrupt(patterns, K)
    else:
        patterns_in = patterns
    states = run_dynamics(patterns_in, weights, iters=iters)
    error = mse(patterns, states)  # How well the Hopefield Net reconstructs the data
    if out_path:  # Optionally save visualizations:
        os.makedirs(out_path)
        show(patterns[0], os.path.join(out_path, 'initial.png'))
        show(states[0], os.path.join(out_path, 'after.png'))
    if return_weights:
        return error, weights, patterns
    else:
        return error


def learn_hopfield_net(patterns):
    # Learns the weights of the Hopfield Net via Hebb's Rule.
    # (6, 128, 128) --> (6, 128*128, 1)
    num_patterns, pattern_res, _ = patterns.shape
    flat_pattern = patterns.reshape((num_patterns, -1))[..., np.newaxis]
    # (6, 128*128, 1) --> (6, 128*128, 128*128)
    rep_pattern = flat_pattern.repeat(pattern_res ** 2, axis=-1)
    weights = (rep_pattern * rep_pattern.transpose(0, 2, 1))  # Hebb's Rule
    I = np.eye(pattern_res ** 2)[np.newaxis]
    weights = (weights * (1 - I)).mean(axis=0)  # Mask-out diagonal weights
    return weights


def forward(states, weights):  # Synchronous dynamics with a zero threshold
    out = np.sign(states @ weights)
    return out


def run_dynamics(states, weights, iters=1, movie=False):
    # Simply runs the dynamics multiple times
    num_patterns, pattern_res, _ = states.shape
    states = states.reshape((num_patterns, -1))
    frames = []
    for _ in range(iters):
        states = forward(states, weights)
        if movie:
            frames.append(states.reshape((num_patterns, pattern_res, pattern_res)))
    states = states.reshape((num_patterns, pattern_res, pattern_res))
    if movie:
        return frames
    else:
        return states


def mse(x, y):
    return ((x - y) ** 2).mean()


def corrupt(patterns, K=0):
    # The input arg K is the expected number of corruptions per image
    K *= patterns.shape[0]
    num_pixels = np.prod(patterns.shape)
    corruptions = np.random.choice([-1] * K + [1] * (num_pixels - K),
                                   size=patterns.shape,
                                   replace=False)
    corrupted = patterns * corruptions
    return corrupted
    

def plot(x, y, name):
    plt.plot(x, y)
    plt.savefig(name)
    plt.clf()


def prepro(img):
    img = 255 * (img + 1) / 2  # [-1, 1] --> [0, 255]
    img = np.clip(img, 0., 255.).astype(np.uint8)
    return img


def show(activations, name, resize=256):
    # Save a visualization of the activations to the Colab folder
    img = prepro(activations)
    Image.fromarray(img).resize((resize, resize), Image.NEAREST).save(name)


def resize_and_np(img, resize=256):
    return np.asarray(Image.fromarray(img).resize((resize, resize), Image.NEAREST))


def make_movie(frames, path):
    # Given frames and a destination, this function makes and saves the video.
    frames = [resize_and_np(prepro(f[0][..., np.newaxis].repeat(3, axis=-1))) for f in reversed(frames)]
    length = len(frames) - 1
    fps = 30
    moviepy.editor.VideoClip(lambda t: frames.pop(), duration=length/fps).write_videofile(path, fps=fps, codec='libx264', bitrate='50M')

"""# Visualizing Hopfield Network Dyanmics

Run the code below to generate a `movie.mp4` file, which you can download and view to see how a random input to the Hopfield Network changes over several iterations in response to a random input pattern.
"""

_, weights, training_patterns = run(num_patterns=25, iters=50, pattern_res=16, return_weights=True)
unseen_patterns = np.random.choice([-1.0, 1.0], size=(1, 16, 16))
frames = run_dynamics(unseen_patterns, weights, iters=60, movie=True)
make_movie(frames, 'movie.mp4')

"""# Exploring the Capacity of a Hopfield Network

With our functioning Hopfield Network implementation, we can begin by seeing how well the Hopfield Net reconstructs the data as we vary the batch size (number of patterns we force it to learn simultaneously). Run the code below to generate a `capacity.png` plot which shows the (mean-squared) reconstruction error as a function of batch size (we use $16\times16$ image patterns). As we can see, for small numbers of patterns (<50), the Hopfield Net does a near-perfect job at reconstructing the data. But, the error rate quickly increases afterwards, before slows after batch size increases beyond ~200. 
"""

max_capacity = 1000
errors = []
for c in tqdm(range(1, max_capacity + 1)):
    error = run(num_patterns=c, iters=50, pattern_res=16)
    errors.append(error)
plot(np.arange(1, max_capacity + 1), errors, 'capacity.png')

"""# Exploring the Robustness of a Hopfield Network

Next, we can look at how well the Hopfield Network can deal with corruptions to the input patterns (note that the corruptions are applied after the weights have been learned). These corruptions simply manifest as random bit flips. Run the code below to generate a plot `robustness.png` which shows reconstruction error as a function of $K$, the expected number of corruptions per-pattern. The Hopfield Network is impressively robust; for ten $16\times16$ patterns, it can still reconstruct near-perfectly for $K<100$. After, though, the error increases rapidly until it reaches the maximum mean-squared error possible (4.0). 
"""

max_corruptions = 1 + 16 ** 2
errors = []
for K in tqdm(range(max_corruptions)):
    error = run(num_patterns=10, pattern_res=16, iters=50, K=K)
    errors.append(error)
plot(np.arange(1, max_corruptions + 1), errors, 'robustness.png')

"""# The Impact of Batch Size on Handling Corruptions

Finally, one might wonder the extent to which batch size (which is the number of patterns being learned) and $K$ (expected number of per-pattern corruptions) interplay. To test this, we fix $K=125$, which corresponded to an "interesting" amount of reconstruction error as shown in the prior plot, and vary batch size from 1 to 256. We can see that when the network only needs to learn a single pattern, it achieves a perfect reconstruct. But, for batch sizes greater than 1, reconstruction error increases rapidly. Interestingly, The error remains roughly constant for batch sizes greater than 1.
"""

max_capacity = 256
errors = []
for c in tqdm(range(1, max_capacity + 1)):
    error = run(num_patterns=c, pattern_res=16, iters=50, K=125)
    errors.append(error)
plot(np.arange(1, max_capacity + 1), errors, 'capacity_and_robustness.png')

"""# Conclusion

In this project, we implemented and investigated Hopfield Networks. We saw that they are robust to sometimes significant amounts of corruption and can store a reasonable number of patterns before incurring significant reconstruction errors. It seems that Hopfield Networks remain quite relevant in machine learning, manifesting as Transformer networks (see this paper for more: https://arxiv.org/abs/2008.02217). Of course, just like Transformers, they incur a painful $O(n^2)$ cost, where $n$ is the number of nodes. It will be interesting to see if they can be made more computationally efficient without sacrificing performance.
"""